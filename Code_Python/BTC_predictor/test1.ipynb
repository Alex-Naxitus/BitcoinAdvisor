{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-34f72f058c99>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-34f72f058c99>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    daily_data[,1] <- factor(daily_data[,1]);\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "library('ggplot2') \n",
    "library('forecast') \n",
    "library('tseries')\n",
    "\n",
    "install.packages(\"rattle\")\n",
    "library(rpart) \n",
    "library(rattle) \n",
    "library(textir) ## needed to standardize the data library(class) ## needed for knn library(ggplot2) # visualization library(ggthemes) # visualization library(scales) # visualization library(dplyr) # data manipulation library(randomForest) # random forest library(corrplot) # correlation library(gridExtra) library(GGally) library(e1071) library(dbplyr) library(sparklyr) library(caret)\n",
    "\n",
    "install.packages(\"randomForest\")\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidyr\")\n",
    "\n",
    "#devtools::install_github(\"tidyverse/tidyr\")\n",
    "library(tidyr)\n",
    "\n",
    "daily_data =read.csv(\"BTC.csv\",header=TRUE, stringsAsFactors=FALSE ) \n",
    "head(daily_data)\n",
    "\n",
    "daily_data[,1] <- factor(daily_data[,1]);\n",
    "daily_data[,1] <- as.Date(daily_data[,1], format = \"%m/%d/%Y\");\n",
    "\n",
    "daily_data$date = as.Date(daily_data$date)\n",
    "\n",
    "cnt_ma30 <- ma(daily_data$close, order=30)\n",
    "\n",
    "daily_data <- daily_data[nrow(daily_data):1,]\n",
    "\n",
    "t <- daily_data$close t<- as.data.frame(t) t<- t [2:1745,] daily_data <- daily_data[1:1744,] daily_data$close1 <- t\n",
    "\n",
    "ggplot() + geom_line(data = daily_data, aes(x = date, y = close, colour = \"Close Value\")) + geom_line(data = daily_data, aes(x = date, y = cnt_ma30, colour = \"Monthly Average\")) + ylab('Close value')\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "index <- sample(1:nrow(daily_data),size = 0.7*nrow(daily_data))\n",
    "\n",
    "train <- daily_data[index,] test <- daily_data [-index,]\n",
    "\n",
    "nrow(train) nrow(test)\n",
    "\n",
    "###Baseline Model\n",
    "best.guess <- mean(train$close) RMSE.baseline <- sqrt(mean((best.guess-test$close)^2)) RMSE.baseline\n",
    "\n",
    "MAE.baseline <- mean(abs(best.guess-test$close)) MAE.baseline\n",
    "\n",
    "###Multiple linear regression\n",
    "lin.reg <- lm(log(close1 +1) ~ open + high + low + close , data = train) summary(lin.reg)\n",
    "\n",
    "test.pred.lin <- exp(predict(lin.reg,test))-1\n",
    "\n",
    "RMSE.lin.reg <- sqrt(mean((test.pred.lin-test$close1)^2)) RMSE.lin.reg\n",
    "\n",
    "MAE.lin.reg <- mean(abs(test.pred.lin-test$close1)) MAE.lin.reg\n",
    "\n",
    "###############Decision Tree\n",
    "rt <- rpart(close1 ~ open + high + low + close, data=train)\n",
    "\n",
    "test.pred.rtree <- predict(rt,test)\n",
    "\n",
    "RMSE.rtree <- sqrt(mean((test.pred.rtree-test$close1)^2)) RMSE.rtree\n",
    "\n",
    "MAE.rtree <- mean(abs(test.pred.rtree-test$close1)) MAE.rtree\n",
    "\n",
    "printcp(rt)\n",
    "\n",
    "min.xerror <- rt$cptable[which.min(rt$cptable[,\"xerror\"]),\"CP\"] min.xerror\n",
    "\n",
    "rt.pruned <- prune(rt,cp = min.xerror) fancyRpartPlot(rt.pruned)\n",
    "\n",
    "test.pred.rtree.p <- predict(rt.pruned,test)\n",
    "\n",
    "RMSE.rtree.pruned <- sqrt(mean((test.pred.rtree.p-test$close1)^2)) RMSE.rtree.pruned\n",
    "\n",
    "MAE.rtree.pruned <- mean(abs(test.pred.rtree.p-test$close1)) MAE.rtree.pruned\n",
    "\n",
    "########Random Forest\n",
    "library(randomForest)\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "rf <- randomForest(close1 ~ open + high + low + close, data = train, importance = TRUE, ntree=1000)\n",
    "\n",
    "which.min(rf$mse)\n",
    "\n",
    "test.pred.forest <- predict(rf,test)\n",
    "\n",
    "RMSE.forest <- sqrt(mean((test.pred.forest-test$close1)^2)) RMSE.forest\n",
    "\n",
    "MAE.forest <- mean(abs(test.pred.forest-test$close1)) MAE.forest\n",
    "\n",
    "#\n",
    "accuracy <- data.frame(Method = c(\"Baseline\",\"Linear Regression\",\"Full tree\",\"Pruned tree\",\"Random forest\"), RMSE = c(RMSE.baseline,RMSE.lin.reg,RMSE.rtree,RMSE.rtree.pruned,RMSE.forest), MAE = c(MAE.baseline,MAE.lin.reg,MAE.rtree,MAE.rtree.pruned,MAE.forest))\n",
    "\n",
    "accuracy$RMSE <- round(accuracy$RMSE,2) accuracy$MAE <- round(accuracy$MAE,2) accuracy\n",
    "\n",
    "all.predictions <- data.frame(actual = test$close, baseline = best.guess, linear.regression = test.pred.lin, full.tree = test.pred.rtree, pruned.tree = test.pred.rtree.p, random.forest = test.pred.forest) head(all.predictions)\n",
    "\n",
    "#\n",
    "install.packages(\"tidyverse\")\n",
    "#Alternatively, install just tidyr:\n",
    "install.packages(\"tidyr\")\n",
    "#Or the the development version from GitHub:\n",
    "install.packages(\"devtools\")\n",
    "devtools::install_github(\"tidyverse/tidyr\")\n",
    "library(tidyr)\n",
    "\n",
    "####################### Visualization - Random Forest- Predicted VS actual\n",
    "test_rf<-as.data.frame(test.pred.forest) test_rf$actual<- test$close\n",
    "\n",
    "ggplot(data = test_rf,aes(x = actual, y =test.pred.forest ))+ geom_point(colour = \"blue\") + geom_abline(intercept = 0, slope = 1, colour = \"red\") + geom_vline(xintercept = 23, colour = \"green\", linetype = \"dashed\")+ ggtitle(\"Predicted vs. Actual, Random Forest\")\n",
    "\n",
    "test_rf_d<- as.data.frame(test_rf) result_rf <- data.frame(test$close1,test_rf_d )\n",
    "\n",
    "error<- test_rf_d$actual - test_rf_d$test.pred.forest\n",
    "\n",
    "result_rf_2 <- data.frame(test_rf_d$actual,test_rf_d$test, error )\n",
    "\n",
    "names(result_rf_2) <- c(\"Actual\", \"Prediction\", \"Error\") hist(result_rf_2$Error)\n",
    "\n",
    "####################### Visualization - Full Tree- Predicted VS actual\n",
    "test_ft<-as.data.frame(test.pred.rtree.p) test_ft$actual<- test$close\n",
    "\n",
    "ggplot(data = test_ft,aes(x = actual, y =test.pred.rtree.p, ))+ geom_point(colour = \"blue\") + geom_abline(intercept = 0, slope = 1, colour = \"red\") + geom_vline(xintercept = 23, colour = \"green\", linetype = \"dashed\")+ ggtitle(\"Predicted vs. Actual, Pruned Tree\")\n",
    "\n",
    "# ####################### Visualization - Linear Regression- Predicted VS actual\n",
    "test_ft<-as.data.frame(test.pred.lin) test_ft$actual<- test$close\n",
    "\n",
    "ggplot(data = test_ft,aes(x = actual, y =test.pred.lin ))+ geom_point(colour = \"blue\") + geom_abline(intercept = 0, slope = 1, colour = \"red\") + geom_vline(xintercept = 23, colour = \"green\", linetype = \"dashed\")+ ggtitle(\"Predicted vs. Actual, Linear Regression\")\n",
    "\n",
    "#\n",
    "all.predictions <- gather(all.predictions,key = model,value = predictions,2:6)\n",
    "\n",
    "head(all.predictions)\n",
    "\n",
    "tail (all.predictions)\n",
    "\n",
    "ggplot(data = all.predictions,aes(x = actual, y = predictions)) +\n",
    "#geom_point(colour = \"blue\") + #geom_abline(intercept = 0, slope = 1, colour = \"red\") + #geom_vline(xintercept = 23, colour = \"green\", linetype = \"dashed\") + #facet_wrap(~ model,ncol = 2) + #coord_cartesian(xlim = c(0,70),ylim = c(0,70)) + #ggtitle(\"Predicted vs. Actual, by model\")\n",
    "\n",
    "#\n",
    "random.forest.predictions <- data.frame(actual = test$close,random.forest = test.pred.forest) random.forest.predictions <- data.frame(actual = test$close,random.forest = test.pred.forest)\n",
    "\n",
    "test.predictions<- data.frame(actual = test$close, baseline = best.guess, linear.regression = test.pred.lin, full.tree = test.pred.rtree, pruned.tree = test.pred.rtree.p, random.forest = test.pred.forest)\n",
    "\n",
    "actual<- test$close Random_Forest <-test.pred.forest pruned_tree <- test.pred.rtree.p full_tree <- test.pred.rtree linear_regression <- test.pred.lin Baseline_m <- best.guess\n",
    "\n",
    "result <- data.frame(actual,Random_Forest , pruned_tree,linear_regression, Baseline_m)\n",
    "\n",
    "########################## NN\n",
    "library('ggplot2') library('forecast') library('tseries') library(rpart) library(rattle) library(textir) ## needed to standardize the data library(class) ## needed for knn library(ggplot2) # visualization library(ggthemes) # visualization library(scales) # visualization library(dplyr) # data manipulation library(randomForest) # random forest library(corrplot) # correlation library(gridExtra) library(GGally) library(e1071) library(dbplyr) library(sparklyr) library(caret) library(tidyr)\n",
    "\n",
    "daily_data =read.csv(\"D:/University/Queens/Term 3/Analytics for Financial markets/Assignments/Project/full.csv\",header=TRUE,stringsAsFactors=FALSE ) head(daily_data)\n",
    "\n",
    "daily_data[,1] <- factor(daily_data[,1]) daily_data[,1] <- as.Date(daily_data[,1], format = \"%m/%d/%Y\")\n",
    "\n",
    "daily_data$date = as.Date(daily_data$date)\n",
    "\n",
    "t <- daily_data$close t<- as.data.frame(t) t<- t [2:1745,] daily_data <- daily_data[1:1744,] daily_data$close1 <- t\n",
    "\n",
    "data <- daily_data\n",
    "\n",
    "data <- daily_data[,2:6]\n",
    "\n",
    "set.seed(500) library(MASS)\n",
    "\n",
    "index <- sample(1:nrow(data),round(0.70*nrow(data))) train <- data[index,] test <- data[-index,]\n",
    "\n",
    "maxs <- apply(data, 2, max) mins <- apply(data, 2, min)\n",
    "\n",
    "scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))\n",
    "\n",
    "train_ <- scaled[index,] test_ <- scaled[-index,]\n",
    "\n",
    "library(neuralnet) n <- names(train_) f <- as.formula(paste(\"close1 ~\", paste(n[!n %in% \"close1\"], collapse = \" + \"))) nn <- neuralnet(f,data=train_,hidden=c(3),linear.output=T) plot(nn)\n",
    "\n",
    "################# predicting\n",
    "pr.nn <- compute(nn,test_[,1:4])\n",
    "\n",
    "pr.nn_ <- pr.nn$net.result*(max(data$close1)-min(data$close1))+min(data$close1) test.r <- (test_$close1)*(max(data$close1)-min(data$close1))+min(data$close1)\n",
    "\n",
    "MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)\n",
    "\n",
    "print(paste(MSE.lm,MSE.nn))\n",
    "\n",
    "#\n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "plot(test$close1,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7) abline(0,1,lwd=2) legend('bottomright',legend='NN',pch=18,col='red', bty='n')\n",
    "\n",
    "#\n",
    "library(boot) set.seed(200) lm.fit <- glm(close1~.,data=data) cv.glm(data,lm.fit,K=10)$delta[1]\n",
    "\n",
    "set.seed(450) cv.error <- NULL k <- 10\n",
    "\n",
    "library(plyr) pbar <- create_progress_bar('text') pbar$init(k)\n",
    "\n",
    "for(i in 1:k){ index <- sample(1:nrow(data),round(0.9*nrow(data))) train.cv <- scaled[index,] test.cv <- scaled[-index,]\n",
    "\n",
    "nn2 <- neuralnet(f,data=train.cv,hidden=c(3),linear.output=T)\n",
    "\n",
    "pr.nn2 <- compute(nn,test.cv[,1:4]) pr.nn2 <- pr.nn2$net.result*(max(data$close1)-min(data$close1))+min(data$close1)\n",
    "\n",
    "test.cv.r <- (test.cv$close1)*(max(data$close1)-min(data$close1))+min(data$close1)\n",
    "\n",
    "cv.error[i] <- sum((test.cv.r - pr.nn2)^2)/nrow(test.cv)\n",
    "\n",
    "pbar$step() }\n",
    "\n",
    "mean(cv.error)\n",
    "\n",
    "boxplot(cv.error,xlab='MSE CV',col='cyan', border='blue',names='CV error (MSE)', main='CV error (MSE) for NN',horizontal=TRUE)\n",
    "\n",
    "#\n",
    "pr.nn_d<- as.data.frame(pr.nn_) result_nn <- data.frame(test$close1,pr.nn_d )\n",
    "\n",
    "error<- test$close1 - pr.nn_d\n",
    "\n",
    "result_nn <- data.frame(test$close1,pr.nn_d, error )\n",
    "\n",
    "names(result_nn) <- c(\"Actual\", \"Prediction\", \"Error\") hist(result_nn$Error)\n",
    "\n",
    "RMSE.nn <- sqrt(mean((result_nn$Prediction-result_nn$Actual)^2)) RMSE.nn\n",
    "\n",
    "MAE.nn <- mean(abs(result_nn$Prediction-result_nn$Actual)) MAE.nn\n",
    "\n",
    "#\n",
    "mean(result_nn$Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
